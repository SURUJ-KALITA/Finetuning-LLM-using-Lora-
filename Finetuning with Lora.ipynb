{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "731d703e-c536-4719-8c51-8934a9cc01ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b7e01c2-1c84-4f2a-bb51-2e0b74abda90",
    "outputId": "316166b4-027a-4756-e9b4-fe88ae75dd4f"
   },
   "source": [
    "# Finetuning with Lora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51f81819-4fbe-45a1-a155-fb56bd401c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.2\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.3.0\n",
      "pandas version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\",\n",
    "        \"numpy\",\n",
    "        \"tiktoken\",\n",
    "        \"torch\",\n",
    "        \"pandas\"\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66edc999-3d91-4a1c-a157-9d056392e8d8",
   "metadata": {
    "id": "66edc999-3d91-4a1c-a157-9d056392e8d8"
   },
   "source": [
    "- No code in this section\n",
    "- Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters\n",
    "- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf",
   "metadata": {
    "id": "8c7017a2-32aa-4002-a2f3-12aac293ccdf"
   },
   "source": [
    "## E.2 Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "def7c09b-af9c-4216-90ce-5e67aed1065c",
    "outputId": "a67a7afe-b401-4463-c731-87025d20f72d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sms_spam_collection\\SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from module import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://github.com/SURUJ-KALITA/Pytorch-/blob/main/sms%5Bspam%5D.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7",
   "metadata": {
    "id": "74c3c463-8763-4cc0-9320-41c7eaad8ab7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from module import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542",
   "metadata": {
    "id": "8681adc0-6f02-4e75-b01a-a6ab75d05542"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57",
   "metadata": {
    "id": "ab7335db-e0bb-4e27-80c5-eea11e593a57"
   },
   "source": [
    "- As a verification step, we iterate through the data loaders and check that the batches contain 8 training examples each, where each training example consists of 120 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4dee6882-4c3a-4964-af15-fa31f86ad047",
    "outputId": "2ae34de1-dd01-4f99-d2c8-ba4dca400754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "Input batch dimensions: torch.Size([8, 120])\n",
      "Label batch dimensions torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for input_batch, target_batch in train_loader:\n",
    "    pass\n",
    "\n",
    "print(\"Input batch dimensions:\", input_batch.shape)\n",
    "print(\"Label batch dimensions\", target_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1",
   "metadata": {
    "id": "5cdd7947-7039-49bf-8a5e-c0a2f4281ca1"
   },
   "source": [
    "- Lastly, let's print the total number of batches in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "IZfw-TYD2zTj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZfw-TYD2zTj",
    "outputId": "4d19ed61-cf7a-4ec4-b822-c847dd1c5d77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 training batches\n",
      "19 validation batches\n",
      "38 test batches\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_loader)} training batches\")\n",
    "print(f\"{len(val_loader)} validation batches\")\n",
    "print(f\"{len(test_loader)} test batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f7022-3e2d-4b46-b434-ac1879b6445a",
   "metadata": {
    "id": "dec9aa4a-ffd2-4d9f-a835-cce1059fe604"
   },
   "source": [
    "##  Initializing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02b3a506-3879-4258-82b5-93a5b6bafa74",
    "outputId": "b8c9b125-bb52-45d3-8071-fa5054dbf5a9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 77.0kiB/s]\n",
      "encoder.json: 100%|███████████████████████████████████████████████████████████████| 1.04M/1.04M [00:05<00:00, 206kiB/s]\n",
      "hparams.json: 100%|████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 30.8kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|███████████████████████████████████████████████| 498M/498M [11:55<00:00, 696kiB/s]\n",
      "model.ckpt.index: 100%|██████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 5.20MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████| 471k/471k [00:01<00:00, 248kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:01<00:00, 304kiB/s]\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from module import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\" \n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252614cd-7ce6-4908-83e6-3761f519904e",
   "metadata": {
    "id": "252614cd-7ce6-4908-83e6-3761f519904e"
   },
   "source": [
    "- To ensure that the model was loaded corrected, let's double-check that it generates coherent text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8b6ce20c-0700-4783-8be0-4cf17c200a7f",
    "outputId": "28ccbca5-8de9-41a0-c093-da00fcbaa91c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you  to the next step.\n",
      "The first step is to find the right\n"
     ]
    }
   ],
   "source": [
    "from module import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you \"   #to be present in the text file of our sms/spm file \n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c00d3d-d9ec-47bf-a007-52c0642b91e2",
   "metadata": {
    "id": "8174b31b-1ab5-4115-b01c-245369da5af3"
   },
   "source": [
    "- Then, we prepare the model for classification finetuning , where we replace the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e255ce91-d73a-4854-90a4-95804928eb16",
   "metadata": {
    "id": "e255ce91-d73a-4854-90a4-95804928eb16"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02e6f057-1383-4ece-8444-0a88e71ac75d",
   "metadata": {
    "id": "02e6f057-1383-4ece-8444-0a88e71ac75d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")\n",
    "\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe",
   "metadata": {
    "id": "8e951cd6-5e42-44d2-b21f-895cb61004fe"
   },
   "source": [
    "- Lastly, let's calculate the initial classification accuracy of the non-finetuned model (we expect this to be around 50%, which means that the model is not able to distinguish between spam and non-spam messages yet reliably)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fc7dd72c-73a2-4881-ade0-0a9605f1ab8c",
    "outputId": "74848515-5a49-4125-fecb-9f4bac23f812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from module import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f56d61-81db-4fbb-b5ac-84417fb3d2f7",
   "metadata": {
    "id": "398a1ec9-e2a1-43d6-bf9f-12ee54b46a7b"
   },
   "source": [
    "##  Parameter-efficient finetuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2ds9ywjMwvIW",
   "metadata": {
    "id": "2ds9ywjMwvIW"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))  # similar to standard weight initialization\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f",
   "metadata": {
    "id": "3e6d5da0-dfce-4808-b89b-29ff333f563f"
   },
   "source": [
    "- To incorporate the original `Linear` layer weights as shown in the figure above, we implement a `LinearWithLoRA` layer below that uses the previously implemented LoRALayer and can be used to replace existing `Linear` layers in a neural network, for example, the self-attention module or feed forward modules in an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "127d3a64-8359-4b21-b056-78d58cc75fe8",
   "metadata": {
    "id": "127d3a64-8359-4b21-b056-78d58cc75fe8"
   },
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1145a90-35ff-462c-820b-15483fa5b051",
   "metadata": {
    "id": "e1145a90-35ff-462c-820b-15483fa5b051"
   },
   "source": [
    "- Note that since we initialize the weight matrix $B$ (`self.B` in `LoRALayer`) with zero values in the LoRA layer, the matrix multiplication between $A$ and $B$ results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 to the original weights does not modify them)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d",
   "metadata": {
    "id": "e98a6d36-7bc9-434c-a7f1-533f26aff06d"
   },
   "source": [
    "- To try LoRA on the GPT model we defined earlier, we define a `replace_linear_with_lora` function to replace all `Linear` layers in the model with the new `LinearWithLoRA` layers\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/appendix-e_compressed/lora-4.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "WlQZ8ygqzN_g",
   "metadata": {
    "id": "WlQZ8ygqzN_g"
   },
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2",
   "metadata": {
    "id": "8c172164-cdde-4489-b7d7-aaed9cc2f5f2"
   },
   "source": [
    "- We then freeze the original model parameter and use the `replace_linear_with_lora` to replace the said `Linear` layers using the code below\n",
    "- This will replace the `Linear` layers in the LLM with `LinearWithLoRA` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbe15350-4da9-4829-9d23-98bbd3d0b1a1",
    "outputId": "fd4c208f-854a-4701-d9d3-9d73af733364"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before: 2,666,528\n",
      "Total trainable parameters after: 0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters before: {total_params:,}\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable parameters after: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "mLk_fPq0yz_u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLk_fPq0yz_u",
    "outputId": "0a93b8fc-05d7-4ace-ee47-e2fc6bdd7d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9",
   "metadata": {
    "id": "b8b6819e-ef7a-4f0d-841a-1b467496bef9"
   },
   "source": [
    "- As we can see, we reduced the number of trainable parameters by almost 50x when using LoRA\n",
    "- Let's now double-check whether the layers have been modified as intended by printing the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1711be61-bb2c-466f-9b5b-24f4aa5ccd9c",
    "outputId": "acff8eca-3775-45a2-b62d-032a986ef037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55",
   "metadata": {
    "id": "c4bbc9d7-65ec-4675-bab8-2e56eb0cfb55"
   },
   "source": [
    "- Based on the model architecture above, we can see that the model now contains our new `LinearWithLoRA` layers\n",
    "- Also, since we initialized matrix $B$ with 0's, we expect the initial model performance to be unchanged compared to before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "DAlrb_I00VEU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAlrb_I00VEU",
    "outputId": "3da44ac4-230b-4358-d996-30b63f0d962a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9db125-7c53-45fd-b6e9-7b4c02212579",
   "metadata": {
    "id": "13735b3e-f0c3-4dba-ae3d-4141b2878101"
   },
   "source": [
    "- Let's now get to the interesting part and finetune the model \n",
    "- The training takes about 15 minutes on a M3 MacBook Air laptop computer and less than half a minute on a V100 or A100 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "wCParRvr0eff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCParRvr0eff",
    "outputId": "ce910a9c-ee89-48bb-bfa6-49c6aee1e450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 1.143, Val loss 0.922\n",
      "Ep 1 (Step 000050): Train loss 0.248, Val loss 0.254\n",
      "Ep 1 (Step 000100): Train loss 0.104, Val loss 0.201\n",
      "Training accuracy: 90.00% | Validation accuracy: 85.00%\n",
      "Ep 2 (Step 000150): Train loss 0.199, Val loss 0.193\n",
      "Ep 2 (Step 000200): Train loss 0.029, Val loss 0.076\n",
      "Ep 2 (Step 000250): Train loss 0.053, Val loss 0.202\n",
      "Training accuracy: 95.00% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.086, Val loss 0.116\n",
      "Ep 3 (Step 000350): Train loss 0.053, Val loss 0.438\n",
      "Training accuracy: 100.00% | Validation accuracy: 92.50%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.096\n",
      "Ep 4 (Step 000450): Train loss 0.011, Val loss 0.149\n",
      "Ep 4 (Step 000500): Train loss 0.074, Val loss 0.020\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 5 (Step 000550): Train loss 0.007, Val loss 0.182\n",
      "Ep 5 (Step 000600): Train loss 0.111, Val loss 0.449\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 33.23 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from module import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c",
   "metadata": {
    "id": "d0c89e82-3aa8-44c6-b046-0b16200b8e6c"
   },
   "source": [
    "- Finally, let's evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bawWGijA0iF3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "bawWGijA0iF3",
    "outputId": "af70782a-d605-4376-fa6c-d33b38979cfa"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from module import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa074723-e3f7-4f7e-a267-855531a037dc",
   "metadata": {
    "id": "aa074723-e3f7-4f7e-a267-855531a037dc"
   },
   "source": [
    "- Note that we previously calculated the accuracy values on 5 batches only via the `eval_iter=5` setting; below, we calculate the accuracies on the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1D2awlEq0gZi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D2awlEq0gZi",
    "outputId": "d603eda1-d912-43eb-ec9c-af6a622510a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.94%\n",
      "Validation accuracy: 97.32%\n",
      "Test accuracy: 96.33%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b257789-430f-4fe9-ba59-43d57baa5f22",
   "metadata": {},
   "source": [
    "# Lora finetuning Successfully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922222e0-21fa-4336-9490-a9b95835dd78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
